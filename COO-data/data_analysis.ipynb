{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use manga109api parser\n",
    "import manga109api\n",
    "manga109_root_dir = \".\"\n",
    "manga109_parser = manga109api.Parser(root_dir=manga109_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 61465 13272\n",
      "total 174023 182 ['!', '.', '?', '~', '★', '☆', '♡', '♥', '♩', '♪', '♫', '♬', '、', '。', 'ぁ', 'あ', 'ぃ', 'い', 'ぅ', 'う', 'ゔ', 'ぇ', 'え', 'ぉ', 'お', 'か', 'が', 'き', 'ぎ', 'く', 'ぐ', 'け', 'げ', 'こ', 'ご', 'さ', 'ざ', 'し', 'じ', 'す', 'ず', 'せ', 'ぜ', 'そ', 'ぞ', 'た', 'だ', 'ち', 'ぢ', 'っ', 'つ', 'づ', 'て', 'で', 'と', 'ど', 'な', 'に', 'ぬ', 'ね', 'の', 'は', 'ば', 'ぱ', 'ひ', 'び', 'ぴ', 'ふ', 'ぶ', 'ぷ', 'へ', 'べ', 'ぺ', 'ほ', 'ぼ', 'ぽ', 'ま', 'み', 'む', 'め', 'も', 'ゃ', 'や', 'ゅ', 'ゆ', 'ょ', 'よ', 'ら', 'り', 'る', 'れ', 'ろ', 'ゎ', 'わ', 'を', 'ん', '゛', '゜', 'ァ', 'ア', 'ィ', 'イ', 'ゥ', 'ウ', 'ヴ', 'ェ', 'エ', 'ォ', 'オ', 'カ', 'ガ', 'キ', 'ギ', 'ク', 'グ', 'ケ', 'ゲ', 'コ', 'ゴ', 'サ', 'ザ', 'シ', 'ジ', 'ス', 'ズ', 'セ', 'ゼ', 'ソ', 'ゾ', 'タ', 'ダ', 'チ', 'ヂ', 'ッ', 'ツ', 'ヅ', 'テ', 'デ', 'ト', 'ド', 'ナ', 'ニ', 'ヌ', 'ネ', 'ノ', 'ハ', 'バ', 'パ', 'ヒ', 'ビ', 'ピ', 'フ', 'ブ', 'プ', 'ヘ', 'ベ', 'ペ', 'ホ', 'ボ', 'ポ', 'マ', 'ミ', 'ム', 'メ', 'モ', 'ャ', 'ヤ', 'ュ', 'ユ', 'ョ', 'ヨ', 'ラ', 'リ', 'ル', 'レ', 'ロ', 'ヮ', 'ワ', 'ン', 'ヶ', '・', 'ー']\n",
      "train 50064 11635\n",
      "train 140631 182 ['!', '.', '?', '~', '★', '☆', '♡', '♥', '♩', '♪', '♫', '♬', '、', '。', 'ぁ', 'あ', 'ぃ', 'い', 'ぅ', 'う', 'ゔ', 'ぇ', 'え', 'ぉ', 'お', 'か', 'が', 'き', 'ぎ', 'く', 'ぐ', 'け', 'げ', 'こ', 'ご', 'さ', 'ざ', 'し', 'じ', 'す', 'ず', 'せ', 'ぜ', 'そ', 'ぞ', 'た', 'だ', 'ち', 'ぢ', 'っ', 'つ', 'づ', 'て', 'で', 'と', 'ど', 'な', 'に', 'ぬ', 'ね', 'の', 'は', 'ば', 'ぱ', 'ひ', 'び', 'ぴ', 'ふ', 'ぶ', 'ぷ', 'へ', 'べ', 'ぺ', 'ほ', 'ぼ', 'ぽ', 'ま', 'み', 'む', 'め', 'も', 'ゃ', 'や', 'ゅ', 'ゆ', 'ょ', 'よ', 'ら', 'り', 'る', 'れ', 'ろ', 'ゎ', 'わ', 'を', 'ん', '゛', '゜', 'ァ', 'ア', 'ィ', 'イ', 'ゥ', 'ウ', 'ヴ', 'ェ', 'エ', 'ォ', 'オ', 'カ', 'ガ', 'キ', 'ギ', 'ク', 'グ', 'ケ', 'ゲ', 'コ', 'ゴ', 'サ', 'ザ', 'シ', 'ジ', 'ス', 'ズ', 'セ', 'ゼ', 'ソ', 'ゾ', 'タ', 'ダ', 'チ', 'ヂ', 'ッ', 'ツ', 'ヅ', 'テ', 'デ', 'ト', 'ド', 'ナ', 'ニ', 'ヌ', 'ネ', 'ノ', 'ハ', 'バ', 'パ', 'ヒ', 'ビ', 'ピ', 'フ', 'ブ', 'プ', 'ヘ', 'ベ', 'ペ', 'ホ', 'ボ', 'ポ', 'マ', 'ミ', 'ム', 'メ', 'モ', 'ャ', 'ヤ', 'ュ', 'ユ', 'ョ', 'ヨ', 'ラ', 'リ', 'ル', 'レ', 'ロ', 'ヮ', 'ワ', 'ン', 'ヶ', '・', 'ー']\n",
      "val 4636 1915\n",
      "val 13656 163 ['!', '.', '?', '~', '★', '☆', '♡', '♥', '♪', '♫', '♬', '。', 'ぁ', 'あ', 'ぃ', 'い', 'ぅ', 'う', 'ゔ', 'ぇ', 'え', 'お', 'か', 'が', 'き', 'ぎ', 'く', 'ぐ', 'け', 'げ', 'こ', 'ご', 'さ', 'ざ', 'し', 'じ', 'す', 'ず', 'せ', 'ぜ', 'そ', 'ぞ', 'た', 'だ', 'ち', 'っ', 'つ', 'て', 'で', 'と', 'ど', 'な', 'に', 'ぬ', 'ね', 'の', 'は', 'ば', 'ぱ', 'ひ', 'び', 'ぴ', 'ふ', 'ぶ', 'ぷ', 'へ', 'べ', 'ぺ', 'ほ', 'ぼ', 'ぽ', 'ま', 'み', 'む', 'も', 'ゃ', 'や', 'ゅ', 'ゆ', 'ょ', 'よ', 'ら', 'り', 'る', 'れ', 'ろ', 'ゎ', 'わ', 'ん', '゛', 'ァ', 'ア', 'ィ', 'イ', 'ゥ', 'ウ', 'ヴ', 'ェ', 'エ', 'ォ', 'オ', 'カ', 'ガ', 'キ', 'ギ', 'ク', 'グ', 'ケ', 'ゲ', 'コ', 'ゴ', 'サ', 'ザ', 'シ', 'ジ', 'ス', 'ズ', 'ゼ', 'ソ', 'ゾ', 'タ', 'ダ', 'チ', 'ッ', 'ツ', 'テ', 'ト', 'ド', 'ナ', 'ニ', 'ハ', 'バ', 'パ', 'ヒ', 'ビ', 'ピ', 'フ', 'ブ', 'プ', 'ヘ', 'ベ', 'ペ', 'ホ', 'ボ', 'ポ', 'ミ', 'ム', 'メ', 'モ', 'ャ', 'ヤ', 'ュ', 'ユ', 'ョ', 'ヨ', 'ラ', 'リ', 'ル', 'ロ', 'ワ', 'ン', '・', 'ー']\n",
      "test 6765 2251\n",
      "test 19736 166 ['!', '.', '?', '~', '★', '☆', '♡', '♥', '♪', '。', 'ぁ', 'あ', 'ぃ', 'い', 'ぅ', 'う', 'ゔ', 'ぇ', 'え', 'ぉ', 'お', 'か', 'が', 'き', 'ぎ', 'く', 'ぐ', 'け', 'げ', 'こ', 'ご', 'さ', 'ざ', 'し', 'じ', 'す', 'ず', 'せ', 'ぜ', 'そ', 'ぞ', 'た', 'だ', 'ち', 'ぢ', 'っ', 'つ', 'て', 'で', 'と', 'ど', 'な', 'に', 'ぬ', 'の', 'は', 'ば', 'ぱ', 'ひ', 'び', 'ぴ', 'ふ', 'ぶ', 'ぷ', 'へ', 'べ', 'ぺ', 'ほ', 'ぼ', 'ぽ', 'ま', 'み', 'む', 'め', 'も', 'ゃ', 'や', 'ゅ', 'ゆ', 'ょ', 'よ', 'ら', 'り', 'る', 'ろ', 'わ', 'ん', '゛', 'ァ', 'ア', 'ィ', 'イ', 'ゥ', 'ウ', 'ヴ', 'ェ', 'エ', 'ォ', 'オ', 'カ', 'ガ', 'キ', 'ギ', 'ク', 'グ', 'ケ', 'ゲ', 'コ', 'ゴ', 'サ', 'ザ', 'シ', 'ジ', 'ス', 'ズ', 'セ', 'ゼ', 'ソ', 'ゾ', 'タ', 'ダ', 'チ', 'ヂ', 'ッ', 'ツ', 'テ', 'デ', 'ト', 'ド', 'ナ', 'ニ', 'ハ', 'バ', 'パ', 'ヒ', 'ビ', 'ピ', 'フ', 'ブ', 'プ', 'ヘ', 'ベ', 'ペ', 'ホ', 'ボ', 'ポ', 'マ', 'ミ', 'ム', 'メ', 'モ', 'ャ', 'ヤ', 'ュ', 'ユ', 'ョ', 'ヨ', 'ラ', 'リ', 'ル', 'レ', 'ロ', 'ワ', 'ン', '・', 'ー']\n"
     ]
    }
   ],
   "source": [
    "# create vocab / char list \n",
    "import re\n",
    "from natsort import natsorted\n",
    "\n",
    "for data_split in [\"total\", \"train\", \"val\", \"test\"]:\n",
    "    with open(f\"{manga109_root_dir}/books_{data_split}.txt\", \"r\") as book_list:\n",
    "        manga_list = book_list.readlines()\n",
    "    \n",
    "    count_polygon = 0\n",
    "    vocab_list = []\n",
    "    for manga_name in manga_list:\n",
    "        manga_name = manga_name.strip()\n",
    "        annotation = manga109_parser.get_annotation(book=manga_name, annotation_type=\"annotations\")\n",
    "        \n",
    "        page_number = len(annotation['page'])\n",
    "        for page_index in range(page_number):\n",
    "            try:\n",
    "                rois = annotation[\"page\"][page_index][\"onomatopoeia\"]\n",
    "                if isinstance(rois, dict):\n",
    "                    rois = [rois]  # for one instance case.\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            for roi in rois:\n",
    "                label = roi[\"#text\"]\n",
    "                vocab_list.append(label)\n",
    "\n",
    "    vocab_set = set(vocab_list)            \n",
    "    sorted_vocab_set = natsorted(vocab_set)\n",
    "\n",
    "    with open(f\"./Onomatopoeia_{data_split}_vocab_set.txt\", \"w\", encoding=\"utf-8\") as vocab_file:\n",
    "        join_sorted_char_set = \"\\n\".join(sorted_vocab_set)\n",
    "        vocab_file.write(join_sorted_char_set)\n",
    "\n",
    "    # len(vocab_list), len(vocab_set), sorted_vocab_set\n",
    "    print(data_split, len(vocab_list), len(vocab_set))\n",
    "    \n",
    "    # char list\n",
    "    char_list = []\n",
    "    for vocab in vocab_list:\n",
    "        for char in vocab:\n",
    "            char_list.append(char)\n",
    "\n",
    "    char_set = set(char_list)\n",
    "    sorted_char_set = natsorted(char_set)\n",
    "    print(data_split, len(char_list), len(char_set), sorted_char_set)\n",
    "\n",
    "    with open(f\"./Onomatopoeia_{data_split}_char_set.txt\", \"w\", encoding=\"utf-8\") as char_file:\n",
    "        join_sorted_char_set = \"\".join(sorted_char_set)\n",
    "        char_file.write(join_sorted_char_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 10004784 3 HealingPlanet_24\n",
      "total 61465 10602 5.797491039426523 6.269096233628894 7.704409030544489 66 82\n",
      "total 37650 14325 9489 61.25437240706093\n",
      "train 10004784 3 HealingPlanet_24\n",
      "train 50064 8763 5.713111947963027 6.3240252476829655 7.725377817622951 66 74\n",
      "train 31232 11080 7751 62.384148290188556\n",
      "val 4636 890 5.2089887640449435 6.390854184641933 7.822068965517241 56 38\n",
      "val 2900 1037 699 62.553925798101815\n",
      "test 6765 949 7.128556375131717 5.779157427937916 7.421262080727686 43 82\n",
      "test 3518 2208 1039 52.00295639320029\n"
     ]
    }
   ],
   "source": [
    "# Count polygon type. \n",
    "# There is only 1 triangle polygon (= polygon consists of 3 points).\n",
    "for data_split in [\"total\", \"train\", \"val\", \"test\"]:\n",
    "    with open(f\"{manga109_root_dir}/books_{data_split}.txt\", \"r\") as book_list:\n",
    "        manga_list = book_list.readlines()\n",
    "\n",
    "    count_pages = 0\n",
    "\n",
    "    count_polygon = 0 \n",
    "    polygon_total_point = 0\n",
    "\n",
    "    count_over_4points = 0\n",
    "    count_over_4points_total_point = 0\n",
    "    count_rectangle = 0\n",
    "    count_quad = 0\n",
    "\n",
    "    max_point = 0\n",
    "    max_polygon_per_page = 0 \n",
    "\n",
    "    for manga_name in manga_list:\n",
    "        manga_name = manga_name.strip()\n",
    "        annotation = manga109_parser.get_annotation(book=manga_name, annotation_type=\"annotations\")\n",
    "        \n",
    "        page_number = len(annotation['page'])\n",
    "        for page_index in range(page_number):\n",
    "            count_pages += 1\n",
    "            polygon_per_page = 0\n",
    "\n",
    "            try:\n",
    "                rois = annotation[\"page\"][page_index][\"onomatopoeia\"]\n",
    "                if isinstance(rois, dict):\n",
    "                    rois = [rois]  # for one instance case.\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            for roi in rois:\n",
    "                count_polygon += 1\n",
    "                polygon_per_page += 1\n",
    "\n",
    "                x_list = [int(roi[attr]) for attr in roi if \"@x\" in attr]\n",
    "                y_list = [int(roi[attr]) for attr in roi if \"@y\" in attr]\n",
    "\n",
    "                polygon_total_point += len(x_list) # all polygon\n",
    "\n",
    "                if len(x_list) != len(y_list):\n",
    "                    assert Error\n",
    "\n",
    "                if len(x_list) < 4: # only 1 triangle polygon\n",
    "                    print(data_split, roi[\"@id\"], len(x_list), f\"{manga_name}_{page_index}\")\n",
    "\n",
    "                elif (x_list[0] == x_list[3] and x_list[1] == x_list[2]) and \\\n",
    "                    (y_list[0] == y_list[1] and y_list[2] == y_list[3]):\n",
    "                    count_rectangle += 1\n",
    "\n",
    "                elif len(x_list) == 4:\n",
    "                    count_quad += 1\n",
    "                else:\n",
    "                    if len(x_list) > max_point:\n",
    "                        max_point = len(x_list)\n",
    "                        # print(\"max point update\", data_split, roi[\"@id\"], len(x_list), f\"{manga_name}_{page_index}\")\n",
    "\n",
    "                    count_over_4points += 1\n",
    "                    count_over_4points_total_point += len(x_list)\n",
    "\n",
    "            if polygon_per_page > max_polygon_per_page:\n",
    "                max_polygon_per_page = polygon_per_page\n",
    "                # print(max_polygon_per_page, manga_name, page_index, roi[\"@id\"])\n",
    "\n",
    "    average_point = polygon_total_point / count_polygon\n",
    "    average_over_4points = count_over_4points_total_point / count_over_4points\n",
    "    over_4points_percentage = count_over_4points/count_polygon * 100\n",
    "    print(data_split, count_polygon, count_pages, count_polygon/count_pages, average_point, average_over_4points, max_point, max_polygon_per_page)\n",
    "    print(data_split, count_over_4points, count_rectangle, count_quad, over_4points_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 10602 2261 0.2132616487455197 2117 132 11 1\n",
      "train 8763 1923 0.21944539541252994 1799 113 10 1\n",
      "val 890 161 0.18089887640449437 152 9 0 0\n",
      "test 949 177 0.18651211801896733 166 10 1 0\n"
     ]
    }
   ],
   "source": [
    "# count link annotations\n",
    "for data_split in [\"total\", \"train\", \"val\", \"test\"]:\n",
    "    with open(f\"{manga109_root_dir}/books_{data_split}.txt\", \"r\") as book_list:\n",
    "        manga_list = book_list.readlines()\n",
    "\n",
    "    count_link = 0\n",
    "    count_link_length2 = 0\n",
    "    count_link_length3 = 0\n",
    "    count_link_length4 = 0\n",
    "    count_link_length5 = 0\n",
    "    total_characters = 0\n",
    "    total_page = 0\n",
    "    for manga_name in manga_list:\n",
    "        manga_name = manga_name.strip()\n",
    "        annotation = manga109_parser.get_annotation(book=manga_name, annotation_type=\"annotations\")\n",
    "        \n",
    "        page_number = len(annotation['page'])\n",
    "        total_page += page_number\n",
    "        for page_index in range(page_number):\n",
    "            id_to_text = {}\n",
    "            count_onomatopoeia_per_page = 0\n",
    "            for annotation_type in [\"onomatopoeia\", \"onomatopoeia_link1\", \"onomatopoeia_link2\"]:\n",
    "                try:\n",
    "                    rois = annotation[\"page\"][page_index][annotation_type]\n",
    "                    if isinstance(rois, dict):\n",
    "                        rois = [rois]  # for one instance case.\n",
    "                        \n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                for roi in rois:\n",
    "                    if annotation_type == \"onomatopoeia\":\n",
    "                        id_to_text[roi[\"@id\"]] = roi[\"#text\"]\n",
    "                        count_onomatopoeia_per_page += 1\n",
    "\n",
    "                    elif \"onomatopoeia_link\" in annotation_type:\n",
    "                        linked_text = \"\"\n",
    "                        link_id_list = [roi[attr] for attr in roi if \"@link\" in attr]\n",
    "                        for id in link_id_list:\n",
    "                            linked_text += id_to_text[id]\n",
    "\n",
    "                        total_characters += len(linked_text)\n",
    "\n",
    "                        length_link_id = len(link_id_list)\n",
    "                        count_link +=1\n",
    "\n",
    "                        if length_link_id == 2:\n",
    "                            # print(length_link_id, manga_name, page_index, roi[\"@id\"])\n",
    "                            count_link_length2 +=1\n",
    "                        \n",
    "                        if length_link_id == 3:\n",
    "                            # print(length_link_id, manga_name, page_index, roi[\"@id\"])\n",
    "                            count_link_length3 +=1\n",
    "                            \n",
    "                        if length_link_id == 4:\n",
    "                            # print(length_link_id, manga_name, page_index, roi[\"@id\"])\n",
    "                            count_link_length4 +=1\n",
    "                            \n",
    "                        if length_link_id == 5:\n",
    "                            # print(length_link_id, manga_name, page_index, roi[\"@id\"])\n",
    "                            count_link_length5 +=1\n",
    "                            \n",
    "    \n",
    "    print(data_split, total_page, count_link, count_link/total_page, count_link_length2, count_link_length3, count_link_length4, count_link_length5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1787 79.0358248562583\n",
      "train 1518 78.93915756630265\n",
      "val 131 81.36645962732919\n",
      "test 138 77.96610169491525\n"
     ]
    }
   ],
   "source": [
    "# count reading link left-to-right\n",
    "for data_split in [\"total\", \"train\", \"val\", \"test\"]:\n",
    "    with open(f\"{manga109_root_dir}/books_{data_split}.txt\", \"r\") as book_list:\n",
    "        manga_list = book_list.readlines()\n",
    "    \n",
    "    count_link = 0\n",
    "    count_left_to_right_link = 0\n",
    "    for manga_name in manga_list:\n",
    "        manga_name = manga_name.strip()\n",
    "        annotation = manga109_parser.get_annotation(book=manga_name, annotation_type=\"annotations\")\n",
    "        \n",
    "        page_number = len(annotation['page'])\n",
    "        for page_index in range(page_number):\n",
    "            id_to_text = {}\n",
    "            id_to_center_x = {}\n",
    "            \n",
    "            for annotation_type in [\"onomatopoeia\", \"onomatopoeia_link1\", \"onomatopoeia_link2\"]:\n",
    "                try:\n",
    "                    rois = annotation['page'][page_index][annotation_type]\n",
    "                    if isinstance(rois, dict):\n",
    "                        rois = [rois]  # for one instance case.\n",
    "                        \n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                for roi in rois:\n",
    "                    if annotation_type == \"onomatopoeia\":\n",
    "                        id_to_text[roi[\"@id\"]] = roi[\"#text\"]\n",
    "                        x_list = [int(roi[attr]) for attr in roi if \"@x\" in attr]\n",
    "                        \n",
    "                        center_x = int(sum(x_list) / len(x_list))\n",
    "                        id_to_center_x[roi[\"@id\"]] = center_x\n",
    "                        \n",
    "                    elif \"onomatopoeia_link\" in annotation_type:\n",
    "                        count_link +=1\n",
    "                        \n",
    "                        link_id_list = [roi[attr] for attr in roi if \"@link\" in attr]\n",
    "                        if id_to_center_x[link_id_list[0]] < id_to_center_x[link_id_list[1]]:\n",
    "                            count_left_to_right_link += 1\n",
    "            \n",
    "    print(data_split, count_left_to_right_link, count_left_to_right_link/count_link*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total\n",
      "{   1: 5845,\n",
      "    2: 26064,\n",
      "    3: 15909,\n",
      "    4: 6713,\n",
      "    5: 3384,\n",
      "    6: 2018,\n",
      "    7: 747,\n",
      "    8: 410,\n",
      "    9: 183,\n",
      "    10: 75,\n",
      "    11: 38,\n",
      "    12: 30,\n",
      "    13: 14,\n",
      "    14: 7,\n",
      "    15: 12,\n",
      "    16: 3,\n",
      "    17: 4,\n",
      "    18: 3,\n",
      "    19: 1,\n",
      "    20: 2,\n",
      "    21: 2,\n",
      "    28: 1}\n",
      "train\n",
      "{   1: 5297,\n",
      "    2: 20941,\n",
      "    3: 12844,\n",
      "    4: 5400,\n",
      "    5: 2732,\n",
      "    6: 1631,\n",
      "    7: 605,\n",
      "    8: 316,\n",
      "    9: 152,\n",
      "    10: 54,\n",
      "    11: 34,\n",
      "    12: 23,\n",
      "    13: 11,\n",
      "    14: 2,\n",
      "    15: 10,\n",
      "    16: 1,\n",
      "    17: 3,\n",
      "    18: 2,\n",
      "    19: 1,\n",
      "    20: 2,\n",
      "    21: 2,\n",
      "    28: 1}\n",
      "val\n",
      "{   1: 237,\n",
      "    2: 2060,\n",
      "    3: 1264,\n",
      "    4: 488,\n",
      "    5: 283,\n",
      "    6: 154,\n",
      "    7: 67,\n",
      "    8: 52,\n",
      "    9: 13,\n",
      "    10: 7,\n",
      "    11: 2,\n",
      "    12: 3,\n",
      "    13: 1,\n",
      "    14: 3,\n",
      "    15: 1,\n",
      "    16: 1}\n",
      "test\n",
      "{   1: 311,\n",
      "    2: 3063,\n",
      "    3: 1801,\n",
      "    4: 825,\n",
      "    5: 369,\n",
      "    6: 233,\n",
      "    7: 75,\n",
      "    8: 42,\n",
      "    9: 18,\n",
      "    10: 14,\n",
      "    11: 2,\n",
      "    12: 4,\n",
      "    13: 2,\n",
      "    14: 2,\n",
      "    15: 1,\n",
      "    16: 1,\n",
      "    17: 1,\n",
      "    18: 1}\n"
     ]
    }
   ],
   "source": [
    "# count characters in each word\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "for data_split in [\"total\", \"train\", \"val\", \"test\"]:\n",
    "    with open(f\"{manga109_root_dir}/books_{data_split}.txt\", \"r\") as book_list:\n",
    "        manga_list = book_list.readlines()\n",
    "    \n",
    "    count_char_length = {}\n",
    "    for manga_name in manga_list:\n",
    "        manga_name = manga_name.strip()\n",
    "        annotation = manga109_parser.get_annotation(book=manga_name, annotation_type=\"annotations\")\n",
    "        \n",
    "        page_number = len(annotation['page'])\n",
    "        for page_index in range(page_number):\n",
    "            for annotation_type in [\"onomatopoeia\"]:\n",
    "                try:\n",
    "                    rois = annotation[\"page\"][page_index][annotation_type]\n",
    "                    if isinstance(rois, dict):\n",
    "                        rois = [rois]  # for one instance case.\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                for roi in rois:\n",
    "                    text = roi[\"#text\"]\n",
    "                    try:\n",
    "                        count_char_length[len(text)] += 1\n",
    "                    except:\n",
    "                        count_char_length[len(text)] = 1\n",
    "    \n",
    "    print(data_split)\n",
    "    pp.pprint(count_char_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 61465 44584 0.7253558935979826\n",
      "train 50064 36383 0.7267297858740812\n",
      "val 4636 3355 0.7236842105263158\n",
      "test 6765 4846 0.7163340724316334\n"
     ]
    }
   ],
   "source": [
    "# Vertical count\n",
    "for data_split in [\"total\", \"train\", \"val\", \"test\"]:\n",
    "    with open(f\"{manga109_root_dir}/books_{data_split}.txt\", \"r\") as book_list:\n",
    "        manga_list = book_list.readlines()\n",
    "    \n",
    "    count_polygon = 0\n",
    "    count_vertical = 0 # the number of images whose height > width\n",
    "\n",
    "    for manga_name in manga_list:\n",
    "        manga_name = manga_name.strip()\n",
    "        annotation = manga109_parser.get_annotation(book=manga_name, annotation_type=\"annotations\")\n",
    "\n",
    "        page_number = len(annotation['page'])\n",
    "        for page_index in range(page_number):\n",
    "            try:\n",
    "                rois = annotation[\"page\"][page_index][\"onomatopoeia\"]\n",
    "                if isinstance(rois, dict):\n",
    "                    rois = [rois]  # for one instance case.\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            for roi in rois:\n",
    "                count_polygon += 1\n",
    "                label = roi[\"#text\"]\n",
    "                x_list = [int(roi[attr]) for attr in roi if \"@x\" in attr]\n",
    "                y_list = [int(roi[attr]) for attr in roi if \"@y\" in attr]\n",
    "\n",
    "                x_min = min(x_list)\n",
    "                y_min = min(y_list)\n",
    "                x_max = max(x_list)\n",
    "                y_max = max(y_list)\n",
    "\n",
    "                width = x_max - x_min\n",
    "                height = y_max - y_min\n",
    "\n",
    "                if height > width:\n",
    "                    count_vertical += 1\n",
    "\n",
    "    print(data_split, count_polygon, count_vertical, count_vertical/count_polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 61465\n",
      "[   ['オ', 891],\n",
      "    ['ザワ', 749],\n",
      "    ['パチ', 687],\n",
      "    ['ワー', 677],\n",
      "    ['ドキ', 365],\n",
      "    ['ン', 347],\n",
      "    ['はっ', 346],\n",
      "    ['?', 333],\n",
      "    ['ゴ', 309],\n",
      "    ['はあ', 308],\n",
      "    ['バッ', 305],\n",
      "    ['ザッ', 296],\n",
      "    ['ハア', 280],\n",
      "    ['ド', 273],\n",
      "    ['ッ', 269],\n",
      "    ['ゴロ', 256],\n",
      "    ['ドン', 247],\n",
      "    ['ガッ', 233],\n",
      "    ['わい', 233],\n",
      "    ['ハァ', 222]]\n",
      "train 50064\n",
      "[   ['オ', 887],\n",
      "    ['ワー', 586],\n",
      "    ['ザワ', 437],\n",
      "    ['ン', 307],\n",
      "    ['パチ', 304],\n",
      "    ['ゴ', 296],\n",
      "    ['?', 289],\n",
      "    ['はっ', 289],\n",
      "    ['ドキ', 285],\n",
      "    ['バッ', 261],\n",
      "    ['ド', 247],\n",
      "    ['ザッ', 246],\n",
      "    ['ッ', 238],\n",
      "    ['ゴロ', 224],\n",
      "    ['ハア', 224],\n",
      "    ['ガッ', 210],\n",
      "    ['ドン', 204],\n",
      "    ['はあ', 202],\n",
      "    ['ア', 199],\n",
      "    ['!', 187]]\n",
      "val 4636\n",
      "[   ['わい', 75],\n",
      "    ['ザワ', 71],\n",
      "    ['パチ', 58],\n",
      "    ['ドキ', 52],\n",
      "    ['ハア', 37],\n",
      "    ['ざわ', 37],\n",
      "    ['ガヤ', 35],\n",
      "    ['ザッ', 35],\n",
      "    ['はっ', 31],\n",
      "    ['ガラッ', 30],\n",
      "    ['ワイ', 29],\n",
      "    ['ドキン', 27],\n",
      "    ['ガタ', 27],\n",
      "    ['プル', 27],\n",
      "    ['どき', 26],\n",
      "    ['バタ', 26],\n",
      "    ['パタ', 24],\n",
      "    ['!', 23],\n",
      "    ['バッ', 22],\n",
      "    ['?', 21]]\n",
      "test 6765\n",
      "[   ['パチ', 325],\n",
      "    ['ザワ', 241],\n",
      "    ['わあー', 126],\n",
      "    ['はあ', 98],\n",
      "    ['わあー・', 80],\n",
      "    ['ワー', 76],\n",
      "    ['プル', 67],\n",
      "    ['ハァ', 66],\n",
      "    ['パタ', 64],\n",
      "    ['フキ', 60],\n",
      "    ['ワイ', 47],\n",
      "    ['わあー・・', 45],\n",
      "    ['ガタ', 43],\n",
      "    ['ドン', 34],\n",
      "    ['ガラ', 34],\n",
      "    ['ビュッ', 33],\n",
      "    ['わい', 31],\n",
      "    ['ドヨ', 31],\n",
      "    ['ドキ', 28],\n",
      "    ['ン', 28]]\n"
     ]
    }
   ],
   "source": [
    "# vocab count\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "for data_split in [\"total\", \"train\", \"val\", \"test\"]:\n",
    "    with open(f\"{manga109_root_dir}/books_{data_split}.txt\", \"r\") as book_list:\n",
    "        manga_list = book_list.readlines()\n",
    "\n",
    "    count_polygon = 0\n",
    "    vocab = {}\n",
    "    for manga_name in manga_list:\n",
    "        manga_name = manga_name.strip()\n",
    "        annotation = manga109_parser.get_annotation(book=manga_name, annotation_type=\"annotations\")\n",
    "        \n",
    "        page_number = len(annotation['page'])\n",
    "        for page_index in range(page_number):\n",
    "            try:\n",
    "                rois = annotation[\"page\"][page_index][\"onomatopoeia\"]\n",
    "                if isinstance(rois, dict):\n",
    "                    rois = [rois]  # for one instance case.\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            for roi in rois:\n",
    "                count_polygon += 1\n",
    "                label = roi[\"#text\"]\n",
    "                if label in vocab:\n",
    "                    vocab[label] += 1\n",
    "                else:\n",
    "                    vocab[label] = 1\n",
    "\n",
    "\n",
    "    print(data_split, count_polygon)\n",
    "    # print(vocab)\n",
    "    vocab_sorted = []\n",
    "    for k, v in sorted(vocab.items(), key=lambda item: item[1], reverse=True)[:20]:\n",
    "        vocab_sorted.append([k, v])\n",
    "\n",
    "    pp.pprint(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
